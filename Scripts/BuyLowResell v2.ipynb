{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b920c139-2fe3-43e8-8f5b-60b979e7729b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 16039 Total Urls - 2024-07-03 11:02:55 PM\n",
      "Valid Urls: 15547 - 100% [▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇]\n",
      "Finished Filtering 15547 valid URLs and 492 filtered URLs - 2024-07-03 11:13:22 PM\n",
      "\n",
      "Attempt 2: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/24520'\n",
      "\n",
      "Attempt 3: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/24520'\n",
      "\n",
      "Attempt 2: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/25031'\n",
      "\n",
      "Attempt 3: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/25031'\n",
      "\n",
      "Attempt 2: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/25030'\n",
      "\n",
      "Attempt 3: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/25030'\n",
      "\n",
      "Attempt 2: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/25047'\n",
      "\n",
      "Attempt 3: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/25047'\n",
      "\n",
      "Attempt 2: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/25008'\n",
      "\n",
      "Attempt 3: Received status code 500 for URL 'https://universalis.app/api/v2/history/Brynhildr/25008'\n",
      "Starting Market Item Request - 15547 Total Items - 156 Total Item Lists - 2024-07-03 11:13:22 PM\n",
      "Attempt 2: RequestException occurred for URL 'https://universalis.app/api/v2/history/Brynhildr/5239,5241,5237,5244,5243,5246,5247,5248,5249,5064,5250,5092,5245,5252,5253,5255,5106,5254,5256,5251,5257,5208,5260,5265,5259,5266,5263,5262,5267,5269,5204,5261,5270,5271,5273,5274,5138,5275,5272,5268,5278,5279,5281,5277,5089,5283,5280,5276,5282,5286,5153,5285,5284,5288,5289,5290,5292,5293,5294,5298,5295,5297,5299,5296,5300,5302,5304,5307,5303,5305,5309,5220,5306,5311,5308,5227,5312,5313,5315,5222,5240,5319,5318,5323,5321,5320,5316,5325,5326,5327,5328,5330,5332,5242,5322,5331,5336,5339,5334,5333?statsWithin=2847774593&entriesWithin=2847775': 500 Server Error: Internal Server Error for url: https://universalis.app/api/v2/history/Brynhildr/5239,5241,5237,5244,5243,5246,5247,5248,5249,5064,5250,5092,5245,5252,5253,5255,5106,5254,5256,5251,5257,5208,5260,5265,5259,5266,5263,5262,5267,5269,5204,5261,5270,5271,5273,5274,5138,5275,5272,5268,5278,5279,5281,5277,5089,5283,5280,5276,5282,5286,5153,5285,5284,5288,5289,5290,5292,5293,5294,5298,5295,5297,5299,5296,5300,5302,5304,5307,5303,5305,5309,5220,5306,5311,5308,5227,5312,5313,5315,5222,5240,5319,5318,5323,5321,5320,5316,5325,5326,5327,5328,5330,5332,5242,5322,5331,5336,5339,5334,5333?statsWithin=2847774593&entriesWithin=2847775\n",
      "Attempt 2: RequestException occurred for URL 'https://universalis.app/api/v2/history/Brynhildr/33321,33315,33313,33318,33310,33311,33322,33324,33326,33316,33320,33314,33597,33323,33600,33598,33317,33295,33319,33603,33604,33325,33655,33605,33601,33659,33661,33599,33608,33257,33656,33610,33667,33286,33611,33657,33660,33602,33144,33606,33607,33612,33658,33695,33696,33668,33687,33671,33693,33710,33609,33258,33673,33670,33688,33672,33707,33684,33675,33669,33769,33708,33774,33674,33691,33779,33819,33662,33839,33776,33843,33840,33842,33849,33777,33778,33771,33770,33709,33844,33873,33772,33877,33847,33879,33850,33841,33848,33872,33881,33768,33888,33773,33775,33706,33869,33880,33818,33882,33889?statsWithin=2847774593&entriesWithin=2847775': 500 Server Error: Internal Server Error for url: https://universalis.app/api/v2/history/Brynhildr/33321,33315,33313,33318,33310,33311,33322,33324,33326,33316,33320,33314,33597,33323,33600,33598,33317,33295,33319,33603,33604,33325,33655,33605,33601,33659,33661,33599,33608,33257,33656,33610,33667,33286,33611,33657,33660,33602,33144,33606,33607,33612,33658,33695,33696,33668,33687,33671,33693,33710,33609,33258,33673,33670,33688,33672,33707,33684,33675,33669,33769,33708,33774,33674,33691,33779,33819,33662,33839,33776,33843,33840,33842,33849,33777,33778,33771,33770,33709,33844,33873,33772,33877,33847,33879,33850,33841,33848,33872,33881,33768,33888,33773,33775,33706,33869,33880,33818,33882,33889?statsWithin=2847774593&entriesWithin=2847775\n",
      "Market Item Lists: 156 - 100%\n",
      "Completed - 2024-07-03 11:26:15 PM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>GatheringType</th>\n",
       "      <th>GatheringLevel</th>\n",
       "      <th>World</th>\n",
       "      <th>Reg_Sale_Vel</th>\n",
       "      <th>NQ_Sale_Vel</th>\n",
       "      <th>HQ_Sale_Vel</th>\n",
       "      <th>Composite_Score</th>\n",
       "      <th>Total_Purchases</th>\n",
       "      <th>Avg_Unit_Price</th>\n",
       "      <th>RN</th>\n",
       "      <th>Buyer</th>\n",
       "      <th>Avg_Qty</th>\n",
       "      <th>Unit_Price</th>\n",
       "      <th>Qty</th>\n",
       "      <th>Overall_Cost</th>\n",
       "      <th>Outlier</th>\n",
       "      <th>HQ_Ind</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Fire Shard</td>\n",
       "      <td>Harvesting</td>\n",
       "      <td>15</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>753</td>\n",
       "      <td>76.557769</td>\n",
       "      <td>1</td>\n",
       "      <td>Adrielle Morrigan</td>\n",
       "      <td>1150.863214</td>\n",
       "      <td>120</td>\n",
       "      <td>1000</td>\n",
       "      <td>120000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/06/25 23:47:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fire Shard</td>\n",
       "      <td>Logging</td>\n",
       "      <td>30</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>753</td>\n",
       "      <td>76.557769</td>\n",
       "      <td>1</td>\n",
       "      <td>Adrielle Morrigan</td>\n",
       "      <td>1150.863214</td>\n",
       "      <td>120</td>\n",
       "      <td>1000</td>\n",
       "      <td>120000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/06/25 23:47:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fire Shard</td>\n",
       "      <td>Mining</td>\n",
       "      <td>20</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>753</td>\n",
       "      <td>76.557769</td>\n",
       "      <td>1</td>\n",
       "      <td>Adrielle Morrigan</td>\n",
       "      <td>1150.863214</td>\n",
       "      <td>120</td>\n",
       "      <td>1000</td>\n",
       "      <td>120000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/06/25 23:47:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Fire Shard</td>\n",
       "      <td>Quarrying</td>\n",
       "      <td>20</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>753</td>\n",
       "      <td>76.557769</td>\n",
       "      <td>1</td>\n",
       "      <td>Adrielle Morrigan</td>\n",
       "      <td>1150.863214</td>\n",
       "      <td>120</td>\n",
       "      <td>1000</td>\n",
       "      <td>120000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/06/25 23:47:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Fire Shard</td>\n",
       "      <td>Harvesting</td>\n",
       "      <td>15</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>26292.193000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>753</td>\n",
       "      <td>76.557769</td>\n",
       "      <td>2</td>\n",
       "      <td>Adrielle Morrigan</td>\n",
       "      <td>1150.863214</td>\n",
       "      <td>92</td>\n",
       "      <td>2000</td>\n",
       "      <td>184000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/06/25 23:47:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458425</th>\n",
       "      <td>44106</td>\n",
       "      <td>Rroneek Chuck</td>\n",
       "      <td>Not Gathered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19</td>\n",
       "      <td>1250.315789</td>\n",
       "      <td>15</td>\n",
       "      <td>Eksu Plosion</td>\n",
       "      <td>20.421053</td>\n",
       "      <td>999</td>\n",
       "      <td>6</td>\n",
       "      <td>5994</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/07/03 10:09:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458426</th>\n",
       "      <td>44106</td>\n",
       "      <td>Rroneek Chuck</td>\n",
       "      <td>Not Gathered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19</td>\n",
       "      <td>1250.315789</td>\n",
       "      <td>16</td>\n",
       "      <td>Eksu Plosion</td>\n",
       "      <td>20.421053</td>\n",
       "      <td>998</td>\n",
       "      <td>19</td>\n",
       "      <td>18962</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/07/03 10:09:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458427</th>\n",
       "      <td>44106</td>\n",
       "      <td>Rroneek Chuck</td>\n",
       "      <td>Not Gathered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19</td>\n",
       "      <td>1250.315789</td>\n",
       "      <td>17</td>\n",
       "      <td>Eksu Plosion</td>\n",
       "      <td>20.421053</td>\n",
       "      <td>998</td>\n",
       "      <td>20</td>\n",
       "      <td>19960</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/07/03 10:09:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458428</th>\n",
       "      <td>44106</td>\n",
       "      <td>Rroneek Chuck</td>\n",
       "      <td>Not Gathered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19</td>\n",
       "      <td>1250.315789</td>\n",
       "      <td>18</td>\n",
       "      <td>Eksu Plosion</td>\n",
       "      <td>20.421053</td>\n",
       "      <td>998</td>\n",
       "      <td>20</td>\n",
       "      <td>19960</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/07/03 10:09:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458429</th>\n",
       "      <td>44106</td>\n",
       "      <td>Rroneek Chuck</td>\n",
       "      <td>Not Gathered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brynhildr</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>11.771718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19</td>\n",
       "      <td>1250.315789</td>\n",
       "      <td>19</td>\n",
       "      <td>Eksu Plosion</td>\n",
       "      <td>20.421053</td>\n",
       "      <td>998</td>\n",
       "      <td>20</td>\n",
       "      <td>19960</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2024/07/03 10:09:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458430 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Item_Id           Name GatheringType GatheringLevel      World  \\\n",
       "0            2     Fire Shard    Harvesting             15  Brynhildr   \n",
       "1            2     Fire Shard       Logging             30  Brynhildr   \n",
       "2            2     Fire Shard        Mining             20  Brynhildr   \n",
       "3            2     Fire Shard     Quarrying             20  Brynhildr   \n",
       "4            2     Fire Shard    Harvesting             15  Brynhildr   \n",
       "...        ...            ...           ...            ...        ...   \n",
       "458425   44106  Rroneek Chuck  Not Gathered            NaN  Brynhildr   \n",
       "458426   44106  Rroneek Chuck  Not Gathered            NaN  Brynhildr   \n",
       "458427   44106  Rroneek Chuck  Not Gathered            NaN  Brynhildr   \n",
       "458428   44106  Rroneek Chuck  Not Gathered            NaN  Brynhildr   \n",
       "458429   44106  Rroneek Chuck  Not Gathered            NaN  Brynhildr   \n",
       "\n",
       "        Reg_Sale_Vel   NQ_Sale_Vel  HQ_Sale_Vel  Composite_Score  \\\n",
       "0       26292.193000  26292.193000          0.0              1.5   \n",
       "1       26292.193000  26292.193000          0.0              1.5   \n",
       "2       26292.193000  26292.193000          0.0              1.5   \n",
       "3       26292.193000  26292.193000          0.0              1.5   \n",
       "4       26292.193000  26292.193000          0.0              1.5   \n",
       "...              ...           ...          ...              ...   \n",
       "458425     11.771718     11.771718          0.0              1.5   \n",
       "458426     11.771718     11.771718          0.0              1.5   \n",
       "458427     11.771718     11.771718          0.0              1.5   \n",
       "458428     11.771718     11.771718          0.0              1.5   \n",
       "458429     11.771718     11.771718          0.0              1.5   \n",
       "\n",
       "        Total_Purchases  Avg_Unit_Price  RN              Buyer      Avg_Qty  \\\n",
       "0                   753       76.557769   1  Adrielle Morrigan  1150.863214   \n",
       "1                   753       76.557769   1  Adrielle Morrigan  1150.863214   \n",
       "2                   753       76.557769   1  Adrielle Morrigan  1150.863214   \n",
       "3                   753       76.557769   1  Adrielle Morrigan  1150.863214   \n",
       "4                   753       76.557769   2  Adrielle Morrigan  1150.863214   \n",
       "...                 ...             ...  ..                ...          ...   \n",
       "458425               19     1250.315789  15       Eksu Plosion    20.421053   \n",
       "458426               19     1250.315789  16       Eksu Plosion    20.421053   \n",
       "458427               19     1250.315789  17       Eksu Plosion    20.421053   \n",
       "458428               19     1250.315789  18       Eksu Plosion    20.421053   \n",
       "458429               19     1250.315789  19       Eksu Plosion    20.421053   \n",
       "\n",
       "        Unit_Price   Qty  Overall_Cost  Outlier  HQ_Ind            Timestamp  \n",
       "0              120  1000        120000    False   False  2024/06/25 23:47:29  \n",
       "1              120  1000        120000    False   False  2024/06/25 23:47:29  \n",
       "2              120  1000        120000    False   False  2024/06/25 23:47:29  \n",
       "3              120  1000        120000    False   False  2024/06/25 23:47:29  \n",
       "4               92  2000        184000    False   False  2024/06/25 23:47:09  \n",
       "...            ...   ...           ...      ...     ...                  ...  \n",
       "458425         999     6          5994    False   False  2024/07/03 10:09:19  \n",
       "458426         998    19         18962    False   False  2024/07/03 10:09:17  \n",
       "458427         998    20         19960    False   False  2024/07/03 10:09:16  \n",
       "458428         998    20         19960    False   False  2024/07/03 10:09:15  \n",
       "458429         998    20         19960    False   False  2024/07/03 10:09:13  \n",
       "\n",
       "[458430 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def check_url(url, retries=3, delay=5, timeout=10):\n",
    "    \"\"\"Function to check if a URL is valid with retry mechanism for timeouts, skipping 404 errors, and pausing every 20 attempts.\"\"\"\n",
    "    status_code_messages = []\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                return url, status_code_messages  # Return URL and status messages list\n",
    "            elif response.status_code == 404:\n",
    "                return None, status_code_messages  # Skip 404 errors without printing\n",
    "            else:\n",
    "                if attempt > 1:\n",
    "                    status_code_messages.append(f\"\\nAttempt {attempt}: Received status code {response.status_code} for URL '{url}'\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "        except requests.Timeout:\n",
    "            if attempt > 1:\n",
    "                status_code_messages.append(f\"\\nAttempt {attempt}: Timeout occurred while fetching URL '{url}'\")\n",
    "            time.sleep(delay)\n",
    "        except requests.RequestException as e:\n",
    "            if attempt > 1:\n",
    "                status_code_messages.append(f\"\\nAttempt {attempt}: An error occurred while fetching URL '{url}': {e}\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        # Pause every 20 attempts\n",
    "        if attempt % 20 == 0:\n",
    "            status_code_messages.append(f\"\\nPausing for 2 seconds after {attempt} attempts...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return None, status_code_messages  # Return None and status messages list if retries are exhausted\n",
    "\n",
    "def filter_urls(urls, max_workers=20):\n",
    "    valid_urls = []\n",
    "    filtered_count = 0\n",
    "    status_messages_buffer = []  # Buffer to collect status messages\n",
    "    print(f\"Filtering {len(urls)} Total Urls - {datetime.datetime.now().strftime('%Y-%m-%d %I:%M:%S %p')}\")\n",
    "\n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_url = {executor.submit(check_url, url): url for url in urls}\n",
    "            for i, future in enumerate(as_completed(future_to_url)):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result, status_messages = future.result()\n",
    "                    if result:\n",
    "                        valid_urls.append(result)\n",
    "                        # Update progress bar only on valid responses\n",
    "                        progress_percent = math.ceil(100 * ((i + 1) / len(urls)))\n",
    "                        sys.stdout.write(f\"\\rValid Urls: {len(valid_urls)} - {progress_percent}% [{'▇' * (progress_percent // 2)}{' ' * (50 - progress_percent // 2)}]\")\n",
    "                        sys.stdout.flush()\n",
    "                    else:\n",
    "                        filtered_count += 1\n",
    "                        status_messages_buffer.extend(status_messages)  # Add messages to the buffer\n",
    "\n",
    "                except TypeError as te:\n",
    "                    status_messages_buffer.append(f\"\\nTypeError occurred while processing URL '{url}': {te}\")\n",
    "                    filtered_count += 1\n",
    "                except Exception as exc:\n",
    "                    status_messages_buffer.append(f\"\\nAn error occurred while processing URL '{url}': {exc}\")\n",
    "                    filtered_count += 1\n",
    "\n",
    "        # Print final completion messages\n",
    "        print(f\"\\nFinished Filtering {len(valid_urls)} valid URLs and {filtered_count} filtered URLs - {datetime.datetime.now().strftime('%Y-%m-%d %I:%M:%S %p')}\")\n",
    "        \n",
    "        # Print status messages\n",
    "        for msg in status_messages_buffer:\n",
    "            print(msg)\n",
    "\n",
    "        return valid_urls, filtered_count  # Return explicitly\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn exception occurred in filter_urls: {e}\")\n",
    "        return [], 0  # Return empty lists if there's an unhandled exception\n",
    "        \n",
    "# Market Item API Pull\n",
    "def fetch_data_with_retry(url, retries=3):\n",
    "    \"\"\"Function to fetch data from API with retry mechanism.\"\"\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt > 1:  # Only print for attempts greater than 1\n",
    "                print(f\"Attempt {attempt}: RequestException occurred for URL '{url}': {e}\")\n",
    "            time.sleep(5)  # Wait 5 seconds before retrying\n",
    "        \n",
    "        # Pause every 20 attempts\n",
    "        if attempt % 20 == 0:\n",
    "            print(f\"Pausing for 2 seconds after {attempt} attempts...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    # Log that retries are exhausted\n",
    "    print(f\"Retries exhausted for URL '{url}'\")\n",
    "    return {}  # Return empty dictionary if retries are exhausted\n",
    "\n",
    "def extract_itemId(url):\n",
    "    return url.rsplit('/', 1)[-1]\n",
    "\n",
    "def  milliseconds_timerange(start_date):\n",
    "    d1 = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    d2 = datetime.datetime.now()  # Current date and time\n",
    "    delta = abs((d2 - d1).total_seconds())  # Difference in seconds\n",
    "    milliseconds = delta * 1000  # Convert seconds to milliseconds\n",
    "    return round(milliseconds)  # Round to the nearest millisecond\n",
    "\n",
    "def second_timerange(start_date):\n",
    "    d1 = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    d2 = datetime.datetime.now()  # Current date and time\n",
    "    delta_seconds = abs((d2 - d1).total_seconds())\n",
    "    return round(delta_seconds)\n",
    "\n",
    "# Define constants\n",
    "private_key = '29309ca885bb4c9096e08ef47213962d19137c17218b4f0a826024807837d435'\n",
    "xivapi_url = 'https://xivapi.com'\n",
    "universalis_api = 'https://universalis.app/api/v2/history'\n",
    "worldDcRegion = '/Brynhildr'\n",
    "timerangeStart = '2024-06-01'\n",
    "statsWithin = f'?statsWithin={milliseconds_timerange(timerangeStart)}'\n",
    "entriesWithin = f'&entriesWithin={second_timerange(timerangeStart)}'\n",
    "timeZone = 'America/Chicago'\n",
    "ENpcBase = 'C:\\\\Users\\\\piked\\\\Documents\\\\Ffxiv Outputs\\\\Ffxiv Scripts\\\\Input Csv\\\\Item.csv'\n",
    "GatheringBase = 'C:\\\\Users\\\\piked\\\\Documents\\\\Ffxiv Outputs\\\\Ffxiv Scripts\\\\Input Csv\\\\GatheringPointBase.csv'\n",
    "FishingBase = 'C:\\\\Users\\\\piked\\\\Documents\\\\Ffxiv Outputs\\\\Ffxiv Scripts\\\\Input Csv\\\\FishingSpot.csv'\n",
    "\n",
    "# SaintCoinach Exported ENpcBase data\n",
    "item_csv = pd.read_csv(ENpcBase, skiprows=1, low_memory=False) # Csv Location\n",
    "item = item_csv.iloc[1:, :].rename(columns={\"#\": \"Item_Id\"})[['Item_Id', 'Name', 'IsUntradable']]\n",
    "item_tble = item[(item['Name'].notnull()) & (item['IsUntradable'] == 'False') & (item['Item_Id'] != '1')].copy()\n",
    "item_tble['urls'] = universalis_api + worldDcRegion + '/' + item_tble['Item_Id'].astype(str)\n",
    "\n",
    "# SaintCoinach Exported Gathering Points data\n",
    "gath_csv = pd.read_csv('C:\\\\Users\\\\piked\\\\Documents\\\\Ffxiv Outputs\\\\Ffxiv Scripts\\\\Input Csv\\\\GatheringPointBase.csv', skiprows=1, low_memory=False)\n",
    "gather = gath_csv.iloc[1:, :][['#','GatheringType', 'GatheringLevel'] + [col for col in gath_csv.columns if 'Item' in col]]\n",
    "fish_csv = pd.read_csv('C:\\\\Users\\\\piked\\\\Documents\\\\Ffxiv Outputs\\\\Ffxiv Scripts\\\\Input Csv\\\\FishingSpot.csv', skiprows=1, low_memory=False)\n",
    "fish = fish_csv.iloc[1:, :][['#','GatheringLevel'] + [col for col in fish_csv.columns if 'Item' in col]]\n",
    "fish['GatheringType'] = 'Fishing'\n",
    "dis_land_df = pd.concat([fish,gather])\n",
    "\n",
    "# Preparing to concat gathering item columns into one\n",
    "dis_land_list = []\n",
    "for i in range(8):\n",
    "    col_name = f'Item[{i}]'\n",
    "    tble = dis_land_df[['#','GatheringType','GatheringLevel', col_name]][dis_land_df[col_name].notnull()].rename(columns={col_name: 'Item'})\n",
    "    dis_land_list.append(tble)\n",
    "\n",
    "dland_df = pd.concat(dis_land_list)[['Item','GatheringType','GatheringLevel']]\n",
    "dland_df = dland_df.groupby(['Item','GatheringType'], as_index=False) \\\n",
    "    .agg(GatheringLevel=pd.NamedAgg(column='GatheringLevel', aggfunc='min'))\n",
    "\n",
    "urls = item_tble['urls'].drop_duplicates().values.tolist()\n",
    "\n",
    "# Filter the URLs\n",
    "valid_urls, filtered_count = filter_urls(urls)\n",
    "\n",
    "# Save the valid URLs to a new list\n",
    "filtered_urls = valid_urls\n",
    "\n",
    "# Extract item IDs from filtered URLs\n",
    "extracted_itemIds = [extract_itemId(url) for url in filtered_urls]\n",
    "\n",
    "# Creating chunks of item IDs\n",
    "n = 100  # Number of item IDs per chunk\n",
    "sep_item_list = [extracted_itemIds[x:x+n] for x in range(0, len(extracted_itemIds), n)]\n",
    "\n",
    "# Creating CSV list\n",
    "csv_list = [\"/\" + \",\".join(map(str, chunk)) for chunk in sep_item_list]\n",
    "\n",
    "# Initialize variables\n",
    "mrkt_hist_list = []\n",
    "items_list_processed = 0\n",
    "data_frames = []\n",
    "\n",
    "print(f\"Starting Market Item Request - {len(extracted_itemIds)} Total Items - {len(csv_list)} Total Item Lists - {datetime.datetime.now().strftime('%Y-%m-%d %I:%M:%S %p')}\")\n",
    "\n",
    "# Processing chunks in a loop\n",
    "for item_id in csv_list:\n",
    "    mrkt_req_json = fetch_data_with_retry(universalis_api + worldDcRegion + item_id + statsWithin + entriesWithin)\n",
    "    mrkt_req_json_itms = mrkt_req_json.get('items', {})\n",
    "    items_list_processed += 1\n",
    "    filtered_itemIDs = [i for i in extracted_itemIds if str(i) in mrkt_req_json_itms and mrkt_req_json_itms[str(i)]['entries']]\n",
    "    for i in filtered_itemIDs:\n",
    "        try:\n",
    "            hist_df = pd.json_normalize(mrkt_req_json_itms[str(i)]).explode('entries').reset_index(drop=True)\n",
    "            hist_df = hist_df.merge(pd.json_normalize(hist_df['entries']), left_index=True, right_index=True).drop('entries', axis=1)\n",
    "            mrkt_hist_list.append(hist_df[[\"itemID\", \"regularSaleVelocity\", \"nqSaleVelocity\", \"hqSaleVelocity\", \"worldName\", \"pricePerUnit\", \"quantity\", \"hq\", \"buyerName\", \"timestamp\"]])\n",
    "        except KeyError:\n",
    "            print(f\"KeyError Entries: {mrkt_req_json}\")\n",
    "            continue\n",
    "    print(f'Market Item Lists: {items_list_processed} - {math.ceil(100 * (items_list_processed / len(csv_list)))}%', end='\\r')\n",
    "\n",
    "print('\\nCompleted - ' + datetime.datetime.now().strftime('%Y-%m-%d %I:%M:%S %p'))\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "hist_table = pd.concat(mrkt_hist_list, ignore_index=True) \\\n",
    "    .rename(columns={\"itemID\" : \"Item_Id\",\"regularSaleVelocity\" : \"Reg_Sale_Vel\",\"nqSaleVelocity\" : \"NQ_Sale_Vel\",\"hqSaleVelocity\" : \"HQ_Sale_Vel\",\"worldName\" :  \"World\",\"pricePerUnit\" : \"Unit_Price\",\"quantity\" : \"Qty\",\"hq\" : \"HQ_Ind\",\"buyerName\" : \"Buyer\",\"timestamp\" : \"Timestamp\"})\n",
    "hist_table = hist_table[hist_table['Reg_Sale_Vel'] > 10]\n",
    "# Sort by Item_Id and Timestamp\n",
    "hist_table.sort_values(by=[\"Item_Id\", \"Timestamp\"], ascending=[True, False], inplace=True)\n",
    "hist_table.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Adding Rank (RN)\n",
    "hist_table['RN'] = hist_table.groupby(\"Item_Id\").cumcount() + 1\n",
    "\n",
    "# Convert Timestamp to desired timezone\n",
    "hist_table['Timestamp'] = pd.to_datetime(hist_table['Timestamp'], unit='s', utc=True).dt.tz_convert(timeZone).dt.strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "# Drop duplicates\n",
    "hist_table.drop_duplicates(inplace=True)\n",
    "\n",
    "# Removing Outlier Prices\n",
    "percentiles = hist_table.groupby(\"Item_Id\")[\"Unit_Price\"].quantile([0.05, 0.95]).unstack(level=1)\n",
    "\n",
    "# Define a fixed threshold for significant deviation\n",
    "threshold = 500  # Adjust this threshold based on your definition of \"significantly different\"\n",
    "\n",
    "# Function to mark outliers\n",
    "def mark_outliers(row):\n",
    "    item_id = row[\"Item_Id\"]\n",
    "    unit_price = row[\"Unit_Price\"]\n",
    "    p05 = percentiles.loc[item_id, 0.05]\n",
    "    p95 = percentiles.loc[item_id, 0.95]\n",
    "    return (unit_price <= p05 - threshold) | (unit_price >= p95 + threshold)\n",
    "\n",
    "# Apply the outlier condition\n",
    "hist_table['Outlier'] = hist_table.apply(mark_outliers, axis=1)\n",
    "\n",
    "# Additional Fields\n",
    "hist_table['Item_Id'] = hist_table['Item_Id'].astype(str)\n",
    "hist_table['Total_Purchases'] = hist_table.groupby('Item_Id')['Item_Id'].transform('count')\n",
    "hist_table['Avg_Unit_Price'] = hist_table.loc[hist_table['Outlier'] != True].groupby('Item_Id')['Unit_Price'].transform('mean')\n",
    "hist_table['Avg_Qty'] = hist_table.loc[hist_table['Outlier'] != True].groupby('Item_Id')['Qty'].transform('mean')\n",
    "hist_table['Overall_Cost'] = hist_table['Unit_Price'] * hist_table['Qty']\n",
    "hist_table['Reg_Sale_Vel_Rank'] = hist_table.groupby('Item_Id')['Reg_Sale_Vel'].rank(method='dense', ascending=False)\n",
    "hist_table['Total_Purchases_Rank'] = hist_table.groupby('Item_Id')['Total_Purchases'].rank(method='dense', ascending=False)\n",
    "hist_table['Avg_Unit_Price_Rank'] = hist_table.groupby('Item_Id')['Avg_Unit_Price'].rank(method='dense', ascending=False)\n",
    "hist_table['Composite_Score'] = (hist_table['Reg_Sale_Vel_Rank'] * 0.7)+(hist_table['Total_Purchases_Rank'] * 0.3)+(hist_table['Avg_Unit_Price_Rank'] * 0.5)\n",
    "\n",
    "# Assuming item_tble is a DataFrame containing item details\n",
    "final_df = hist_table.merge(item_tble, on='Item_Id') \\\n",
    "    .merge(dland_df,how='left',left_on='Name',right_on='Item')[['Item_Id', 'Name', 'GatheringType', 'GatheringLevel', 'World', 'Reg_Sale_Vel', 'NQ_Sale_Vel', 'HQ_Sale_Vel', 'Composite_Score', 'Total_Purchases', 'Avg_Unit_Price', 'RN', 'Buyer', 'Avg_Qty', 'Unit_Price', 'Qty', 'Overall_Cost', 'Outlier', 'HQ_Ind', 'Timestamp']]\n",
    "final_df['GatheringType'] = final_df['GatheringType'].fillna('Not Gathered')\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8aeac52d-f7a0-41d0-a604-8efbc51a8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\piked\\\\Documents\\\\Ffxiv Outputs\\\\Ffxiv Scripts\\\\Output Csv\\\\')\n",
    "\n",
    "final_df.to_csv('Hi_Mrkt_Vel v2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
